{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21519633-5a99-442d-a829-af8d14e14907",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Иллюстрация предобработки признаков для датасета Amazon\n",
    "<blockquote>\n",
    "    <p>Показанная ниже генерация признаков вынесена в <a href=\"https://github.com/AlgoMathITMO/sber-simulator/blob/experiments-new/experiments/Amazon/amazon.py\" title=\"amazon.py\">файл</a>, \n",
    "        и выполняется вместе с разбиение данных для последующего применения в экспериментах в \n",
    "        <a href=\"https://github.com/AlgoMathITMO/sber-simulator/blob/experiments-new/experiments/Amazon/train_test_split.ipynb\" title=\"traint_test_split.ipynb\">ноутбуке</a>.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50661592-1283-4b46-b862-f344fd5af351",
   "metadata": {},
   "source": [
    "### $\\textbf{Содержание}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b55a0d-e718-4b6e-af7c-8ac9b82f2518",
   "metadata": {},
   "source": [
    "### $\\textbf{I. Генерация признаков}$\n",
    "### Для каждой пары предложение-пользователь генерация признаков проводится следующим образом:\n",
    "#### - среднее арифметическое векторных представлений слов описания атрибута $\\it{review}$ - $\\it{embedding}=\\{\\it{embedding}_i\\}_{i=0}^{299} \\in \\mathbb{R}^{300}$;\n",
    "#### - значение полезности берется из атрибута полезности $\\it{helpfulness}$;\n",
    "#### - оценка пользователя на предложение также является первоначальным атрибутом $\\it{score}$;\n",
    "\n",
    "### Из этих признаков генерируются следующие признаки пользователей:\n",
    "#### - среднее арифметическое векторных представлений текстовых описаний товаров, оцененных пользователем $\\it{embedding}=\\{\\it{embedding}_i\\}_{i=0}^{299} \\in \\mathbb{R}^{300}$; \n",
    "#### - средний рейтинг, выставленный пользователем $\\it{rating\\_avg} \\in \\mathbb{R}^+$;\n",
    "#### - количество предложений, оцененных пользователем $\\it{rating\\_cnt} \\in \\mathbb{N}$;\n",
    "#### - средняя полезность, выставленная пользователем $\\it{helpfulness} \\in \\mathbb{R}^+$;\n",
    "\n",
    "### Из этих же самых признаков генерируются признаки предложений:\n",
    "#### - среднее арифметическое векторных представлений всех текстовых описаний предложения пользователями $\\it{embedding}=\\{\\it{embedding}_i\\}_{i=0}^{299} \\in \\mathbb{R}^{300}$; \n",
    "#### - средний рейтинг предложения $\\it{rating\\_avg} \\in \\mathbb{R}^+$;\n",
    "#### - количество оценко, выставленных пользователями $\\it{rating\\_cnt} \\in \\mathbb{N}$;\n",
    "#### - средняя полезность предложения $\\it{helpfulness} \\in \\mathbb{R}^+$;\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8534e75-38e1-47c3-9484-a11abc5e8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/home/agurov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/home/agurov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /data/home/agurov/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib widget\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "words = set([w.lower() for w in words])\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.downloader import load\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import expr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0ff43-da56-4cd6-b81d-56f489fd01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_parquet = \"all.parquet\"\n",
    "SPARK_MASTER_URL = 'spark://spark:7077'\n",
    "SPARK_DRIVER_HOST = 'jupyterhub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb221508-9be5-4c21-a557-b2b744ada138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd261ceb-9f40-4db9-ba15-ec676212b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(\n",
    "    mode\n",
    ") -> SparkSession: \n",
    "    \"\"\"\n",
    "    The function creates spark session\n",
    "    :param mode: session mode\n",
    "    :type mode: int\n",
    "    :return: SparkSession\n",
    "    :rtype: SparkSession\n",
    "    \"\"\"\n",
    "    if mode == 1:        \n",
    "        conf = SparkConf().setAll([\n",
    "        ('spark.master', SPARK_MASTER_URL),\n",
    "        ('spark.driver.bindAddress', '0.0.0.0'),\n",
    "        ('spark.driver.host', SPARK_DRIVER_HOST),\n",
    "        ('spark.driver.blockManager.port', '12346'),\n",
    "        ('spark.driver.port', '12345'),\n",
    "        ('spark.driver.memory', '8g'), #4\n",
    "        ('spark.driver.memoryOverhead', '2g'),\n",
    "        ('spark.executor.memory', '14g'), #14\n",
    "        ('spark.executor.memoryOverhead', '2g'),\n",
    "        ('spark.app.name', 'simulator'),\n",
    "        ('spark.submit.deployMode', 'client'),\n",
    "        ('spark.ui.showConsoleProgress', 'true'),\n",
    "        ('spark.eventLog.enabled', 'false'),\n",
    "        ('spark.logConf', 'false'),\n",
    "        ('spark.network.timeout', '10000000'),\n",
    "        ('spark.executor.heartbeatInterval', '10000000'),\n",
    "        ('spark.sql.shuffle.partitions', '4'),\n",
    "        ('spark.default.parallelism', '4'),\n",
    "        (\"spark.kryoserializer.buffer\",\"1024\"),\n",
    "        ('spark.sql.execution.arrow.pyspark.enabled', 'true'),\n",
    "        ('spark.rpc.message.maxSize', '1000'),\n",
    "        (\"spark.driver.maxResultSize\", \"2g\")    \n",
    "        ])\n",
    "        spark = SparkSession.builder\\\n",
    "            .config(conf=conf)\\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    elif mode == 0:\n",
    "        spark = SparkSession.builder\\\n",
    "            .appName('simulator')\\\n",
    "            .master('local[4]')\\\n",
    "            .config('spark.sql.shuffle.partitions', '4')\\\n",
    "            .config('spark.default.parallelism', '4')\\\n",
    "            .config('spark.driver.extraJavaOptions', '-XX:+UseG1GC')\\\n",
    "            .config('spark.executor.extraJavaOptions', '-XX:+UseG1GC')\\\n",
    "            .config('spark.sql.autoBroadcastJoinThreshold', '-1')\\\n",
    "            .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\\\n",
    "            .getOrCreate()\n",
    "               \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a83e65a-4caf-4b4c-b6c3-f14cdc5a85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleaning and preprocessing of the tags text with help of regular expressions.\n",
    "    :param text: initial text\n",
    "    :type text: str\n",
    "    :return: cleaned text\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \",text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+$\", \"\", text)\n",
    "    text = re.sub(r\"^\\s+\", \"\", text)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def string_embedding(arr: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Processing each word in the string with word2vec and return their aggregation (mean).\n",
    "    \n",
    "    :param arr: words\n",
    "    :type text: List[str]\n",
    "    :return: average vector of word2vec words representations\n",
    "    :rtype: np.ndarray\n",
    "    \n",
    "    Arguments:\n",
    "    --string: string of tags.\n",
    "    \n",
    "    Return:\n",
    "    --vec: vector of string embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    vec = 0\n",
    "    cnt = 0\n",
    "    for i in arr:\n",
    "        try:\n",
    "            vec += w2v_model[i]\n",
    "            cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "    if cnt == 0:\n",
    "        vec = np.zeros((300,))\n",
    "    else:\n",
    "        vec /= cnt\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908feb9b-45dc-4e8d-8d69-0a5d046bcdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/agurov/.conda/envs/sber3.8/lib/python3.8/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@sf.pandas_udf(StringType(), sf.PandasUDFType.SCALAR)\n",
    "def clean_udf(str_series):\n",
    "    \"\"\"\n",
    "    pandas udf of the clean_text function\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for x in str_series:\n",
    "        x_procc = clean_text(x)\n",
    "        result.append(x_procc)\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338ee681-4f81-41f0-9d58-46eb2d93bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sf.pandas_udf(ArrayType(DoubleType()), sf.PandasUDFType.SCALAR)\n",
    "def embedding_udf(str_series):\n",
    "    \"\"\"\n",
    "    pandas udf of the string_embedding function\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for x in str_series:\n",
    "        x_procc = string_embedding(x)\n",
    "        result.append(x_procc)\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dbb9f-b50d-4062-be58-547cef3070f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7edfcd3-e79c-4834-9fce-14f82479d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800959ec-5f56-494c-9b5f-176421fde601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(file_name_parquet)\n",
    "df_sp = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9fe69-6953-421a-ab6d-3405f3507125",
   "metadata": {},
   "source": [
    "#### I. Генерация признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e3675-2eed-49b0-becb-9f2f752662d1",
   "metadata": {},
   "source": [
    "Обработка теста атрибута $\\it{review}$ и перевод его в векторное представление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f74f17-1db0-437a-af1d-bc9f5ad4bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procc = df_sp.withColumnRenamed(\"item_id\", \"item_idx\")\\\n",
    "             .withColumnRenamed(\"user_id\", \"user_idx\")\\\n",
    "             .withColumn(\"review_clean\", clean_udf(sf.col(\"review\"))).drop(\"review\", \"__index_level_0__\")\n",
    "df_procc = tokenizer.transform(df_procc).drop(\"review_clean\")\n",
    "df_procc = remover.transform(df_procc).drop(\"tokens\")\n",
    "df_procc = df_procc.withColumn(\"embedding\", embedding_udf(sf.col(\"tokens_clean\"))).drop(\"tokens_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e4ce9-cb8f-4973-afd3-3854200666e5",
   "metadata": {},
   "source": [
    "Аггрегирование признаков пользователей и признаков предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff5861-e459-4a6c-a6e9-ee88075231fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp.cache()\n",
    "df_items = df_sp.groupby(\"item_idx\").agg(sf.array(*[sf.mean(sf.col(\"embedding\")[i]) for i in range(300)]).alias(\"embedding\"),\n",
    "                                            sf.mean(\"helpfulness\").alias(\"helpfulness\"),\n",
    "                                            sf.mean(\"score\").alias(\"rating_avg\"),\n",
    "                                            sf.count(\"score\").alias(\"rating_cnt\"))\n",
    "\n",
    "df_users = df_sp.groupby(\"user_idx\").agg(sf.array(*[sf.mean(sf.col(\"embedding\")[i]) for i in range(300)]).alias(\"embedding\"),\n",
    "                                            sf.mean(\"helpfulness\").alias(\"helpfulness\"),\n",
    "                                            sf.mean(\"score\").alias(\"rating_avg\"),\n",
    "                                            sf.count(\"score\").alias(\"rating_cnt\"))\n",
    "\n",
    "df_rating = df_sp.groupby(\"user_idx\", \"item_idx\", \"timestamp\").agg(sf.mean(\"score\").alias(\"relevance\"), sf.count(\"score\").alias(\"rating_cnt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da3c0b-da60-47a6-b53e-a2aeaf50380e",
   "metadata": {},
   "source": [
    "Раскрытие массива $\\it{embedding}$ в несколько колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4feea49-8a6f-44c3-8bda-1b73a39cc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.select(['item_idx', 'helpfulness', 'rating_avg', 'rating_cnt']+[expr('embedding[' + str(x) + ']') for x in range(0, 300)])\n",
    "new_colnames = ['item_idx', 'helpfulness', 'rating_avg', 'rating_cnt'] + ['w2v_' + str(i) for i in range(0, 300)] \n",
    "df_items = df_items.toDF(*new_colnames)\n",
    "\n",
    "df_users = df_users.select(['user_idx', 'helpfulness', 'rating_avg', 'rating_cnt']+[expr('embedding[' + str(x) + ']') for x in range(0, 300)])\n",
    "new_colnames = ['user_idx', 'helpfulness', 'rating_avg', 'rating_cnt'] + ['w2v_' + str(i) for i in range(0, 300)] \n",
    "df_users = df_users.toDF(*new_colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab1027-47b5-44dd-bb45-2779b94fab51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865cd92-ed3c-4bb6-9e32-8e192b581c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-sber3.8]",
   "language": "python",
   "name": "conda-env-.conda-sber3.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
