{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3723f0-15da-4e41-baff-d1fac30b6da1",
   "metadata": {},
   "source": [
    "# Иллюстрация  разбиения датасета Amazon на train/test/split\n",
    "<blockquote>\n",
    "    <p>Предобработки признаков для датасета Amazon, используемая здесь вынесена в \n",
    "       <a href=\"https://github.com/AlgoMathITMO/sber-simulator/blob/experiments-new/experiments/Amazon/amazon.py\" title=\"amazon.py\">файл</a>,\n",
    "       иллюстрация её работы продемонстрирована в <a href=\"https://github.com/AlgoMathITMO/sber-simulator/blob/experiments-new/experiments/Amazon/amazon_processing.ipynb\" \n",
    "       title=\"amazon_preprocessing\">ноутбуке</a>.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854aa20-25e6-4468-935b-46fbb1dfb0a3",
   "metadata": {},
   "source": [
    "### $\\textbf{Содержание}$:\n",
    "\n",
    "### $\\textbf{I. Загрузка данных }$\n",
    "#### - Чтение данных с диска;\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4aafa-b8c5-4e7d-b8c0-3a79ea3fae26",
   "metadata": {},
   "source": [
    "### $\\textbf{II. Разбиение данных для эксперимента}$\n",
    "### Для разбиения данных на $\\it{train/test/split}$ производится деление исходного датасета *df_rating* по квантилям атрибута $\\it{timestamp}$, $\\mathbb{q}$ для генерации признаков:\n",
    "#### $\\it{rating}_{t}$ = *df_rating*$[0, \\mathbb{q}_{t}]$, где $\\mathbb{q}_{train}=0.5$, $\\mathbb{q}_{val}=0.75$, $\\mathbb{q}_{test}=1$:\n",
    "#### - $\\it{rating}_{train}$ = *df_rating*$[0, 0.5]$;\n",
    "#### - $\\it{rating}_{val}$ = *df_rating*$[0, 0.75]$;\n",
    "#### - $\\it{rating}_{test}$ = *df_rating*$[0, 1]$;\n",
    "### Далее для каждого из промежутков {$\\it{rating}_{train}$, $\\it{rating}_{val}$, $\\it{rating}_{test}$} генерируются соответствующие им признаки пользователей и предложений:\n",
    "#### - $\\it{items}_{t}$, $\\it{users}_{t}$, $\\it{rating}_{t}$ = data_processing(movies, $\\it{rating}_{t}$, tags), $t \\in \\{\\it{train}, \\it{val}, \\it{test}\\}$;\n",
    "### После чего формируются окончательные рейтинги:\n",
    "#### - $\\it{rating}_{train}$ = $\\it{rating}_{train}$ = *df_rating*$[0, 0.5]$;\n",
    "#### - $\\it{rating}_{val}$ = $\\it{rating}_{val}$[$\\mathbb{q}>\\mathbb{q}_{train}$] = *df_rating*$(0.5, 0.75]$;\n",
    "#### - $\\it{rating}_{test}$ = $\\it{rating}_{test}$[$\\mathbb{q}>\\mathbb{q}_{val}$] = *df_rating*$(0.75, 1]$;\n",
    "\n",
    "<blockquote>\n",
    "    <p>То есть, если для генерации признаков для валидационного набора данных мы используем временные метки с 0 по 0.75 квантиль, то в качестве рейтингов мы возьмем оценки\n",
    "       только с 0.5 по 0.75 квантили. Аналогично для тестового набора: все временные метки для генерации признаков, но в качестве рейтингов только оценки с 0.75 по 1\n",
    "       квантили.</p>\n",
    "</blockquote>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23f90f-056c-44c3-b18c-13e8064a4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/home/agurov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/home/agurov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /data/home/agurov/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "/data/home/agurov/.conda/envs/sber3.8/lib/python3.8/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import tqdm\n",
    "\n",
    "from amazon import data_processing, get_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4cb15-e49f-4ae8-a569-14703cf9f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_parquet = \"all.parquet\"\n",
    "save_path = \"hdfs://namenode:9000/Sber_data/Amazon/final_data\"\n",
    "\n",
    "spark = get_spark_session(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625524c-b878-4dc8-84bc-b41d7ffddd1c",
   "metadata": {},
   "source": [
    "I. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec812a-a519-409e-9258-5d331cc9633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(file_name_parquet)\n",
    "df_sp = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09161ae-21f5-4a78-8c42-376c20009a4a",
   "metadata": {},
   "source": [
    "### II. Разбиение данных для эксперимента"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d7c9e-b0b4-4bdc-9878-a0e46b5832bd",
   "metadata": {},
   "source": [
    "### Разбиение df_rating на train/test/validation части по квантилям timestamp:\n",
    "####  - train [0, 0.5]\n",
    "####  - validation [0, 0.75]\n",
    "####  - test [0, 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dbd2348-4188-4a68-8fe6-b0d463703e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile: 0.5 - 1189555200.0, 0.75 - 1301443200.0\n"
     ]
    }
   ],
   "source": [
    "q_50, q_75 = df_sp.approxQuantile(\"timestamp\", [0.5, 0.75], 0)\n",
    "print(f\"Quantile: 0.5 - {q_50}, 0.75 - {q_75}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1aa2c3-29fe-4278-9b33-08014ede15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_procc.filter(sf.col(\"timestamp\") <= q_50)\n",
    "df_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41174c08-956e-47a6-881c-1164a6f4f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_procc.filter(sf.col(\"timestamp\") <= q_75)\n",
    "df_val.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd08ad6-4d4f-489d-b530-a2a4ce26fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_procc\n",
    "df_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef300d-d957-443c-b8f9-413354e574ea",
   "metadata": {},
   "source": [
    "#### Генерация признаков по временным промежуткам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427898bd-811b-4c96-b85a-0003929bfc69",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581f3cd-2ee3-4dae-bcba-bc227f319463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_train, df_items_train, df_rating_train = data_processing(df_train)\n",
    "\n",
    "df_items_train.write.parquet(os.path.join(save_path, r'train/items.parquet'))\n",
    "df_users_train.write.parquet(os.path.join(save_path, r'train/users.parquet'))\n",
    "df_rating_train.write.parquet(os.path.join(save_path, r'train/rating.parquet'))\n",
    "\n",
    "df_train.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abd8d2-eec1-4aab-9de5-51da78fe1530",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df22394-6d7f-4499-bedd-2d35399de706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_val, df_items_val, df_rating_val = data_processing(df_val)\n",
    "\n",
    "df_rating_val = df_rating_val.filter(sf.col(\"timestamp\") > q_50)\n",
    "df_items_val.write.parquet(os.path.join(save_path, r'val/items.parquet'))\n",
    "df_users_val.write.parquet(os.path.join(save_path, r'val/users.parquet'))\n",
    "df_rating_val.write.parquet(os.path.join(save_path, r'val/rating.parquet'))\n",
    "\n",
    "df_val.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ebc6f5-4f65-4b99-a261-dbf02ea7d132",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa40c1f-c8ce-48c2-9d79-e58abc78752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_test, df_items_test, df_rating_test = data_processing(df_test)\n",
    "\n",
    "df_rating_test = df_rating_test.filter(sf.col(\"timestamp\") > q_75)\n",
    "df_items_test.write.parquet(os.path.join(save_path, r'val/items.parquet'))\n",
    "df_users_test.write.parquet(os.path.join(save_path, r'val/users.parquet'))\n",
    "df_rating_test.write.parquet(os.path.join(save_path, r'val/rating.parquet'))\n",
    "\n",
    "df_test.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-sber3.8]",
   "language": "python",
   "name": "conda-env-.conda-sber3.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
